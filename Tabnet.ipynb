{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabnet: Attentive Tabular Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                        # pandas dataframes\n",
    "import numpy  as np                                        # numpy arrays\n",
    "import torch                                               # Pytorch: neural network backend\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier      # Tabnet: neural network models for tabular learning\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer    # semisupervised pre-training\n",
    "from sklearn.model_selection import KFold                  # k-fold cross-validation\n",
    "from sklearn.metrics import accuracy_score                 # classification accuracy\n",
    "from sklearn.metrics import cohen_kappa_score              # loss function appropriate for imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\n",
    "\n",
    "# wisconsin diagnostic breast cancer data set\n",
    "wdbc = pd.read_csv(url, header=None, usecols=range(1, 32)) # remove zeroth column (irrelevant index feature)\n",
    "label = list(wdbc.columns)[0]                              # column name of the class label\n",
    "features = list(wdbc.columns)[1:]                          # column names of the features\n",
    "y = pd.factorize(wdbc[label])[0]                           # y-vector (with target encoding)\n",
    "X = wdbc[features]                                         # design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = X.to_numpy()     # encode for PyTorch\n",
    "y = torch.tensor(y)  # encode for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "        1.189e-01],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "        8.902e-02],\n",
       "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "        8.758e-02],\n",
       "       ...,\n",
       "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "        7.820e-02],\n",
       "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "        1.240e-01],\n",
       "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "        7.039e-02]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "X                    # preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "y[0:40] # preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Early stopping occured at epoch 20 with best_epoch = 10 and best_val_0_unsup_loss = 4.52838\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n",
      "\n",
      "Early stopping occured at epoch 268 with best_epoch = 168 and best_val_0_logloss = 0.09929\n",
      "Best weights from best epoch are automatically used!\n",
      "True class labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "Predicted class labels: [0 0 1 0 0 0 0 1 1 0]\n",
      "Kappa: 0.9432082364662903\n",
      "Accuracy: 0.9736842105263158\n",
      "\n",
      "Early stopping occured at epoch 17 with best_epoch = 7 and best_val_0_unsup_loss = 5.16916\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n",
      "\n",
      "Early stopping occured at epoch 214 with best_epoch = 114 and best_val_0_logloss = 0.11177\n",
      "Best weights from best epoch are automatically used!\n",
      "True class labels: tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "Predicted class labels: [0 0 0 0 0 1 0 0 0 0]\n",
      "Kappa: 0.9395546129374337\n",
      "Accuracy: 0.9736842105263158\n",
      "\n",
      "Early stopping occured at epoch 15 with best_epoch = 5 and best_val_0_unsup_loss = 52700.02734\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n",
      "\n",
      "Early stopping occured at epoch 194 with best_epoch = 94 and best_val_0_logloss = 0.23849\n",
      "Best weights from best epoch are automatically used!\n",
      "True class labels: tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1])\n",
      "Predicted class labels: [1 0 0 0 0 1 1 1 0 1]\n",
      "Kappa: 0.7898089171974523\n",
      "Accuracy: 0.9035087719298246\n",
      "\n",
      "Early stopping occured at epoch 18 with best_epoch = 8 and best_val_0_unsup_loss = 5.4086\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n",
      "\n",
      "Early stopping occured at epoch 321 with best_epoch = 221 and best_val_0_logloss = 0.24086\n",
      "Best weights from best epoch are automatically used!\n",
      "True class labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "Predicted class labels: [0 0 0 0 0 1 1 1 1 0]\n",
      "Kappa: 0.7629116117850954\n",
      "Accuracy: 0.8947368421052632\n",
      "\n",
      "Early stopping occured at epoch 10 with best_epoch = 0 and best_val_0_unsup_loss = 339236.96875\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n",
      "\n",
      "Early stopping occured at epoch 337 with best_epoch = 237 and best_val_0_logloss = 0.21835\n",
      "Best weights from best epoch are automatically used!\n",
      "True class labels: tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0])\n",
      "Predicted class labels: [0 0 1 1 1 1 0 0 0 0]\n",
      "Kappa: 0.8141447368421053\n",
      "Accuracy: 0.911504424778761\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)  # define k-fold cross-validation plan\n",
    "\n",
    "log_loss_array = []  # initialize list of out-of-fold prediction log loss\n",
    "kappa_array    = []  # initialize list of out-of-fold prediction kappa\n",
    "accuracy_array = []  # initialize list of out-of-fold prediction accuracy\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    X_train, X_valid = X[train_index], X[test_index]   # X variables\n",
    "    y_train, y_valid = y[train_index], y[test_index]   # y vector\n",
    "    \n",
    "    # feature scaling\n",
    "    X_train = (X_train - np.min(X_train))/np.ptp(X_train)\n",
    "    X_valid = (X_valid - np.min(X_valid))/np.ptp(X_valid)\n",
    "\n",
    "    # semisupervised pre-training\n",
    "    unsupervised_model = TabNetPretrainer(\n",
    "        verbose=0, seed=42,\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=2e-2),\n",
    "        mask_type='entmax' # \"sparsemax\"\n",
    "        )\n",
    "\n",
    "    # obtain pre-trained model weights\n",
    "    unsupervised_model.fit(\n",
    "        X_train=X_train,\n",
    "        eval_set=[X_valid],\n",
    "        pretraining_ratio=0.8,\n",
    "        )\n",
    "    \n",
    "    # define the model\n",
    "    classifier = TabNetClassifier(\n",
    "                    verbose=0, seed=42,\n",
    "                    optimizer_fn=torch.optim.Adam,     # Adam optimizer\n",
    "                    optimizer_params=dict(lr=2e-2)     # default learning rate\n",
    "                    )\n",
    "    \n",
    "    classifier.fit(X_train=X_train, y_train=y_train,   # in-sample data\n",
    "               eval_set=[(X_valid, y_valid)],          # out-of-fold data\n",
    "               patience=100,                           # stopping threshhold if no improvement\n",
    "               max_epochs=2000,                        # maximum training epochs\n",
    "               eval_metric=[\"logloss\"],                # loss function to optimize\n",
    "               from_unsupervised=unsupervised_model)   # load pre-trained model weights      \n",
    "\n",
    "    log_loss_array.append(classifier.best_cost)\n",
    "    kappa_array.append(cohen_kappa_score(y1=y_valid, y2=classifier.predict(X_valid)))\n",
    "    accuracy_array.append(accuracy_score(y_true=y_valid, y_pred=classifier.predict(X_valid)))\n",
    "    \n",
    "    print(\"True class labels:\", y_valid[0:10])\n",
    "    print(\"Predicted class labels:\", classifier.predict(X_valid[0:10]))\n",
    "    print(\"Kappa:\", cohen_kappa_score(y1=y_valid, y2=classifier.predict(X_valid)))\n",
    "    print(\"Accuracy:\", accuracy_score(y_true=y_valid, y_pred=classifier.predict(X_valid)))\n",
    "\n",
    "    # ignore UserWarning: CUDA initialization: Found no NVIDIA driver on your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.09929120780087256,\n",
       " 0.11177492534523226,\n",
       " 0.23848992636418156,\n",
       " 0.24085603396505145,\n",
       " 0.21835174915591915]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "log_loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.9432082364662903,\n",
       " 0.9395546129374337,\n",
       " 0.7898089171974523,\n",
       " 0.7629116117850954,\n",
       " 0.8141447368421053]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "kappa_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.9736842105263158,\n",
       " 0.9736842105263158,\n",
       " 0.9035087719298246,\n",
       " 0.8947368421052632,\n",
       " 0.911504424778761]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "accuracy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8499256230456753"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "sum(kappa_array) / len(kappa_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9314236919732961"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "sum(accuracy_array) / len(accuracy_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bitb6e054adbab245c882617ace8b00a156"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}